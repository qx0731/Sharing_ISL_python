{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.9 Lab: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow.keras as tk\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing import sequence\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.models import Model\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "In Python, we use keras as the DL interface with backend Tensorflow.\n",
    "pip install keras \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9.1 A Single Layer Network on the Hitters Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this exercise, we will use Hitters data set to predict the salary of a player\n",
    "# I will skip the linear regression and Lasso part since we covered them in previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data and take a look at the data and split it into train and test \n",
    "# I copied the code from chapter (because of laziness :- )\n",
    "Hitters = pd.read_csv('data/Hitters.csv', header=0, na_values='NA')\n",
    "Hitters = Hitters.dropna().reset_index(drop=True) # drop the observation with NA values and reindex the obs from 0\n",
    "dummies = pd.get_dummies(Hitters[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "y = Hitters.Salary  # the response variable \n",
    "X_prep = Hitters.drop (['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "X = pd.concat([X_prep,  dummies[['League_A', 'Division_E', 'NewLeague_A']]], axis=1)\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.66)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the DL model is similar to other models implemeted in sklearn. \n",
    "# we first define the model, then fit the model, and finally predict the result\n",
    "\n",
    "\"\"\"\n",
    "I am listing out the hyperameters to be tuned. \n",
    "From this simple example with only one layer, we could get a sense of the number of hyperameters in NN.\n",
    "\n",
    "Actually the number of the hyperameters gets exponentially larger as the number of layers increases.\n",
    "\"\"\"\n",
    "# define the model.model.add\n",
    "dropout_rate = 0.4\n",
    "first_layout = 50\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "activation = 'relu'\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'rmsprop'\n",
    "metrics = ['mae']\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dropout(rate=dropout_rate, input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(first_layout, activation=activation))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "# we can use the model.summary() to see the structure of the model\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "# model.evaluate returns the loss value & metrics values for the model in test mode.\n",
    "mse_test, mae_test = model.evaluate(X_test, y_test)\n",
    "print('Test mse: %.3f, Test mae: %.3f' % (mse_test, mae_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9.2 A Multilayer Network on the MNIST Digit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could load the MNIST data set from keras.datasets\n",
    "# keras.datasets also contains other well-known datasets, such as cifar10, fashion_mnist, etc.\n",
    "(X_train, y_train), (X_test, y_test) = tk.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "255\n",
      "(60000,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# since the data set if for image, so each image is a 28*28 matrix.\n",
    "print(X_train.shape)\n",
    "print(np.max(X_train))\n",
    "\n",
    "# the y_train the group label for the training data\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us plot some of the images\n",
    "for i in range(9):\n",
    "\t# define subplot\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\tplt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us reshape each image (i.e. matrix) to a vector\n",
    "X_train = X_train.reshape((X_train.shape[0], 28*28)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], 28*28)).astype('float32')\n",
    "\"\"\" \n",
    "We know that each pixel has its unique color code and also we know that it has a maximum value of 255. \n",
    "To perform Machine Learning, it is important to convert all the values from 0 to 255 for every pixel to \n",
    "a range of values from 0 to 1. The simplest way is to divide the value of every pixel by 255 to get the \n",
    "values in the range of 0 to 1.\n",
    "\"\"\"\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim = 28 * 28, activation= 'relu'))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model and fit the model \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_model = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test))\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot the metric history \n",
    "history_dict = history_model.history\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history_dict['accuracy'], color='black', label='train')\n",
    "plt.plot(history_dict['val_accuracy'], color='orange', label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\"\"\" \n",
    "One thing to notice is the test accuracy is actually higher than the training accuracy. \n",
    "It is kind of uncommon - if you have a good explaination on this, let me know. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9.3 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tk.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the data shape, compare to MNIST, this dataset is also image but with \n",
    "# different channels, i.e. squared image with 32×32 pixels and three color channels\n",
    "print(X_train.shape)\n",
    "print(np.max(X_train))\n",
    "\n",
    "# the y_train the group label for the training data\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few samples\n",
    "for i in range(9):\n",
    "\t# define subplot\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\tplt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the workflow is similar to before\n",
    "# we prepare the data \n",
    "# then define the model architecture \n",
    "# then fit the model \n",
    "# then exam the model performance \n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a 3 block (each block contains conv layer and pooling layer) VGG architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# then define the output classifer part of the model \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(100, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model and fit the model \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_model = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test), verbose=0)\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot the metric history \n",
    "history_dict = history_model.history\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history_dict['accuracy'], color='black', label='train')\n",
    "plt.plot(history_dict['val_accuracy'], color='orange', label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as before, we can add in dropout \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "# compile the model and fit the model \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_model = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test), verbose=0)\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: %.3f' % acc)\n",
    "\"\"\" \n",
    "From the result, we can see that adding in dropout decreases the test accuracy from \n",
    "37.6% to 34.1%. This is mainly due to the underfit of the model.\n",
    "\n",
    "Dropout helps in the case of overfitting, to see the benefit of the dropout, we would need to \n",
    "increase epochs to a large number (~ 1000). \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some other regularization technique is not mentioned in this lab is weight decay (i.e. similar to the concept of Lasso and Ridge)\n",
    "In this setup, we can add regularization to the weights using the syntax 'kernel_regularizer'\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "Some other regularization methods are data augmentation and early stopping. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9.4 Using Pretrained CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using the examples in the book, I choose a VGG model \n",
    "# to make it simple, I used VGG16. There are other pretrained models in this keras.applications class\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# load model\n",
    "model = VGG16()\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# fun to check the number of paramters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image from file\n",
    "image = load_img('./data/dog_test.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[0])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "there are other usage on pre-trained models. The example we showed above is to use the model directly to do the prediction. \n",
    "\n",
    "Other usage could be as a feature extracter: one example is below. Then we could run other simpler model on top of these features.\n",
    "\"\"\"\n",
    "# load model\n",
    "model = VGG16()\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "# get extracted features\n",
    "features = model.predict(image)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9.5 IMDb Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tk.datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the data shape, compare to MNIST, this dataset is also image but with \n",
    "# different channels, i.e. squared image with 32×32 pixels and three color channels\n",
    "print(X_train.shape)\n",
    "\n",
    "# the y_train the group label for the training data\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X_train]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can cap the length of each review at 500\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to before, we can build a model. Here I skipped the OHE case, and directly went to use the \n",
    "# embedding \n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save compute, I only run 5 epochs\n",
    "history_model = model.fit(X_train, y_train, epochs=5,\n",
    "                          batch_size=512, validation_data=(X_test, y_test))\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot the metric history \n",
    "history_dict = history_model.history\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history_dict['accuracy'], color='black', label='train')\n",
    "plt.plot(history_dict['val_accuracy'], color='orange', label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# similar to before, we can add dropout and other regularizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9.6 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to expedite the run， I used vocabulary_size = 5000 here \n",
    "vocabulary_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = tk.datasets.imdb.load_data(num_words=vocabulary_size)\n",
    "\n",
    "# we can cap the length of each review at 500\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100)) # this is the key part of this section: the LSTM layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save compute, I only run 10 epochs，this takes ~10 mins on my machine. \n",
    "# In theory, this RNN setup should produce better results than the CNN setup.\n",
    "history_model = model.fit(X_train, y_train, epochs=10,\n",
    "                          batch_size=512, validation_data=(X_test, y_test))\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot the metric history \n",
    "history_dict = history_model.history\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history_dict['accuracy'], color='black', label='train')\n",
    "plt.plot(history_dict['val_accuracy'], color='orange', label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\"\"\" \n",
    "I hope by now, you have a good understanding of the NN model and the reason why those models are becoming popular.\n",
    "Those model are easy to define, easy to train, easy to use, and easy to apply to new usecases.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Chapter 10"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4548a0e672c5b3a287feee7b2962606840aa548749d1830ef724408652b0c250"
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
